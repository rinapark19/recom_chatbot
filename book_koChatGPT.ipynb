{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNwb8YMdBHbIO828U7KHeTX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**환경설정**"],"metadata":{"id":"QXaA0ZRSNEM9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WXujtlhLKqOz"},"outputs":[],"source":["## setup(1min)\n","# torch 버전 다운. torch>=2.0 에선 colosalai가 동작안함\n","!pip uninstall torch -y\n","!pip install torch==1.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n","\n","import torch\n","\n","# for transformers, 최신버전은 에러발생\n","!pip install transformers==4.28.1\n","\n","# for ColossalAI\n","!pip install colossalai==0.2.7\n","\n","# setup data\n","!git clone https://github.com/airobotlab/KoChatGPT\n","!mv KoChatGPT/data_kochatgpt .\n","!mv KoChatGPT/img .\n","\n","%cd KoChatGPT/colossalai_ChatGPT_230319/\n","!pip install .\n","%cd ../../\n","\n","# setup library\n","!pip install openai\n","!pip install langchain==0.0.113\n","!pip install pandas>=1.4.1"]},{"cell_type":"markdown","source":["**SFT**"],"metadata":{"id":"qSS7QynANGhX"}},{"cell_type":"code","source":["# import\n","import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from datasets import load_dataset\n","import transformers\n","from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline\n","from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n","from copy import deepcopy\n","from torch.optim import Adam\n","from transformers import AutoTokenizer, BloomTokenizerFast\n","from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n","import pandas as pd\n","import argparse\n","import copy\n","import logging\n","import json\n","from dataclasses import dataclass, field\n","\n","def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n","    \"\"\"Collects the state dict and dump to disk.\"\"\"\n","    state_dict = trainer.model.state_dict()\n","    if trainer.args.should_save:\n","        cpu_state_dict = {key: value.cpu() for key, value in list(state_dict.items())}\n","        del state_dict\n","        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa"],"metadata":{"id":"CVz_DkeGNCK4","executionInfo":{"status":"ok","timestamp":1687137089843,"user_tz":-540,"elapsed":11459,"user":{"displayName":"박채린","userId":"04948549455345468544"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Define argument\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument(\"--data_path_1_SFT\", type=str, default=\"./data_kochatgpt/kochatgpt_1_SFT.jsonl\")\n","parser.add_argument(\"--data_path_1_SFT_add\", type=str, default='./data_kochatgpt/instruction_prompt.json')\n","parser.add_argument(\"--model_name\", type=str, default='skt/kogpt2-base-v2')\n","parser.add_argument(\"--max_epochs\", type=int, default=2)\n","parser.add_argument(\"--train_batch_size\", type=int, default=8)\n","parser.add_argument(\"--output_dir\", type=str, default='./output_1_SFT')\n","\n","args = parser.parse_args(args=[])\n","\n","print(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VbXD0nKhNMjR","executionInfo":{"status":"ok","timestamp":1687139086069,"user_tz":-540,"elapsed":509,"user":{"displayName":"박채린","userId":"04948549455345468544"}},"outputId":"717b1fe2-b0e8-4d46-b8e3-54d521da7737"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(data_path_1_SFT='./data_kochatgpt/kochatgpt_1_SFT.jsonl', data_path_1_SFT_add='./data_kochatgpt/instruction_prompt.json', model_name='skt/kogpt2-base-v2', max_epochs=2, train_batch_size=8, output_dir='./output_1_SFT')\n"]}]},{"cell_type":"code","source":["# data config\n","IGNORE_INDEX = -100\n","DEFAULT_PAD_TOKEN = \"[PAD]\"\n","DEFAULT_EOS_TOKEN = \"</s>\"\n","DEFAULT_BOS_TOKEN = \"</s>\"\n","DEFAULT_UNK_TOKEN = \"</s>\"\n","PROMPT_DICT = {\n","    \"prompt_input\": (\n","        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n","        \"아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\\n\\n\"\n","        \"Write a response that appropriately completes the request.\\n요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n","        \"### Instruction(명령어):\\n{prompt}\\n\\n### Input(입력):\\n{input}\\n\\n### Response(응답):\"\n","    ),\n","    \"prompt_no_input\": (\n","        \"Below is an instruction that describes a task.\\n\"\n","        \"아래는 작업을 설명하는 명령어입니다.\\n\\n\"\n","        \"Write a response that appropriately completes the request.\\n명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n","        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n","    ),\n","}"],"metadata":{"id":"QqWIQKXGQArE","executionInfo":{"status":"ok","timestamp":1687137829056,"user_tz":-540,"elapsed":3,"user":{"displayName":"박채린","userId":"04948549455345468544"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Prepare data\n","from typing import Optional, Dict, Sequence\n","\n","class SFT_dataset(Dataset):\n","  def __init__(self, data_path_1_SFT: str, data_path_1_SFT_add:str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n","    super(SFT_dataset, self).__init__()\n","    logging.warning(\"Loading data...\")\n","\n","    # Format\n","    pattern_instruction = 'prompt'\n","    pattern_input = \"input\"\n","    pattern_output = 'completion'\n","\n","    # Load dataset\n","    with open(data_path_1_SFT, \"r\", encoding=\"utf-8-sig\") as json_file:\n","      list_data_dict = json.load(json_file)\n","      if verbose:\n","        print((list_data_dict[0]))\n","\n","    with open(data_path_1_SFT_add, \"r\", encoding=\"utf-8-sig\") as json_file:\n","      list_data_dict_1 = json.load(json_file)\n","      if verbose:\n","        print((list_data_dict_1[0]))\n","\n","    # list_data_dict.append(list_data_dict_1)\n","    # print((list_data_dict[-1]))\n","\n","    list_data_dict += list_data_dict_1\n","\n","    # 데이터셋 만들기\n","    prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n","\n","    # 입력\n","    sources = []\n","    for example in list_data_dict:\n","      if example.get(pattern_input, \"\") != \"\":\n","        tmp = prompt_input.formap_map(example)\n","      else:\n","        tmp = prompt_no_input.format_map(example)\n","      sources.append(tmp)\n","\n","    # 출력\n","    targets = []\n","    for example in list_data_dict:\n","      targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n","\n","    if verbose:\n","      idx = 0\n","      print((sources[idx]))\n","      print((targets[idx]))\n","      print(\"Tokenizing inputs... This may take some time\")\n","\n","    # 입력 + 출력 = examples\n","    examples = [s + t for s, t in zip(sources, targets)]\n","\n","    # data tokenized\n","    sources_tokenized = self._tokenize_fn(sources, tokenizer)\n","    examples_tokenized = self._tokenize_fn(examples, tokenizer)\n","\n","    # 학습은 target 부분만\n","    input_ids = examples_tokenized['input_ids']\n","    labels = copy.deepcopy(input_ids)\n","    for label, source_len in zip(labels, sources_tokenized['input_ids_lens']):\n","      label[:source_len] = IGNORE_INDEX\n","\n","    data_dict = dict(input_ids = input_ids, labels=labels)\n","\n","    self.input_ids = data_dict['input_ids']\n","    self.labels = data_dict['labels']\n","\n","  def _tokenize_fn(self, strings:Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n","    tokenized_list = [\n","         tokenizer(\n","            text,\n","            return_tensors = \"pt\",\n","            padding=\"longest\",\n","            max_length=tokenizer.model_max_length,\n","            truncation=False,\n","         )\n","         for text in strings\n","    ]\n","    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n","    input_ids_lens = labels_lens = [\n","        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n","    ]\n","    return dict(\n","         input_ids = input_ids,\n","         labels=labels,\n","         input_ids_lens = input_ids_lens,\n","         labels_lens = labels_lens,\n","    )\n","  def __len__(self):\n","    return len(self.input_ids)\n","\n","  def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n","    return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n","\n","@dataclass\n","class DataCollatorForSupervisedDataset(object):\n","  tokenizer: transformers.PreTrainedTokenizer\n","\n","  def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n","    input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n","    input_ids = torch.nn.utils.rnn.pad_sequence(\n","        input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n","    )\n","    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n","    return dict(\n","        input_ids=input_ids,\n","        labels=labels,\n","        attention_mask = input_ids.ne(self.tokenizer.pad_token_id),\n","    )\n","\n","from transformers import PreTrainedTokenizerFast\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n","                                                    bos_token=\"</s>\", eos_token='</s>',\n","                                                    pad_token='<pad>', mask_token='<mask>')\n","train_dataset = SFT_dataset(data_path_1_SFT=args.data_path_1_SFT, data_path_1_SFT_add=args.data_path_1_SFT_add, tokenizer=tokenizer)\n","eval_dataset = None\n","data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MeGgZ6BZQEh5","executionInfo":{"status":"ok","timestamp":1687140475350,"user_tz":-540,"elapsed":19659,"user":{"displayName":"박채린","userId":"04948549455345468544"}},"outputId":"3eebb940-e212-4f73-a281-2da9cea71f77"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n","The class this function is called from is 'PreTrainedTokenizerFast'.\n","WARNING:root:Loading data...\n"]}]},{"cell_type":"code","source":["# 모델 준비\n","model = AutoModelForCausalLM.from_pretrained(args.model_name)\n","tokenizer = transformers.AutoTokenizer.from_pretrained(\n","    args.model_name,\n","    padding_side=\"right\",\n","    model_max_length=512,\n",")\n","tokenizer.add_special_tokens(\n","    {\n","        'eos_token': DEFAULT_EOS_TOKEN,\n","        'bos_token': DEFAULT_BOS_TOKEN,\n","        'unk_token': DEFAULT_UNK_TOKEN,\n","    }\n",")\n","tokenizer.pad_token = tokenizer.eos_token"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-LtPrIroX94e","executionInfo":{"status":"ok","timestamp":1687140104429,"user_tz":-540,"elapsed":3665,"user":{"displayName":"박채린","userId":"04948549455345468544"}},"outputId":"2047b442-11bc-44d4-f5a2-859cc41ed2f8"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}]},{"cell_type":"code","source":["!pip install --upgrade accelerate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ma2qsSvoZfF-","executionInfo":{"status":"ok","timestamp":1687140318885,"user_tz":-540,"elapsed":11086,"user":{"displayName":"박채린","userId":"04948549455345468544"}},"outputId":"5b10efdb-d372-43eb-c25c-2adcc643b551"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting accelerate\n","  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.13.1+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.20.3\n"]}]},{"cell_type":"code","source":["# 학습\n","training_args = TrainingArguments(\n","    output_dir=\"./test\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=2,\n","    per_device_train_batch_size=2,\n","    per_device_eval_batch_size=2,\n","    eval_steps=3,\n","    save_steps=1000,\n","    warmup_steps=5,\n","    prediction_loss_only=True\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n",")\n","\n","trainer.train()\n","trainer.save_state()\n","safe_save_model_for_hf_trainer(trainer=trainer, output_dir=args.output_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":885},"id":"z1WQv9E8YxMm","executionInfo":{"status":"ok","timestamp":1687142698727,"user_tz":-540,"elapsed":2199214,"user":{"displayName":"박채린","userId":"04948549455345468544"}},"outputId":"0dd87a6d-7b45-4007-8fe6-9bb6c03a02b8"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12040' max='12040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12040/12040 36:33, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>3.298000</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>3.179900</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>3.076700</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>3.091400</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>3.031800</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>2.984900</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>2.935400</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>2.913700</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>2.898200</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>2.896000</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>2.794100</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>2.837300</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>2.191700</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>2.159100</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>2.157200</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>2.158000</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>2.104800</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>2.164800</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>2.108500</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>2.072100</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>2.116800</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>2.098900</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>2.048900</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>2.026800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}]},{"cell_type":"code","source":["# 테스트\n","generator = pipeline('text-generation', model=args.output_dir, tokenizer=tokenizer)\n","\n","generation_args = dict(\n","    num_beams=4,\n","    repetition_penalty=2.0,\n","    no_repeat_ngram_size=4,\n","    eos_token_id=375,\n","    max_new_tokens=64,\n","    do_sample=True,\n","    top_k=50,\n","    early_stopping=True\n",")"],"metadata":{"id":"_dZrwWbyZZ5Y","executionInfo":{"status":"ok","timestamp":1687142961460,"user_tz":-540,"elapsed":3690,"user":{"displayName":"박채린","userId":"04948549455345468544"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["def recom_chat(input: str):\n","  list_prompt = [input]\n","  list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({\"prompt\": tmp}) for tmp in list_prompt]\n","\n","  list_result = generator(list_prompt, **generation_args)\n","\n","  for prompt, result in zip(list_prompt, list_result):\n","    print((\"#\" * 70))\n","    print((\"completion: %s\"%(result[0]['generated_text'])))"],"metadata":{"id":"l6x3FaTGiqW7","executionInfo":{"status":"ok","timestamp":1687142948474,"user_tz":-540,"elapsed":534,"user":{"displayName":"박채린","userId":"04948549455345468544"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["recom_chat(\"초등학생이 읽을 만한 책 추천해 줘\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"93deiVb8jdQi","executionInfo":{"status":"ok","timestamp":1687142973667,"user_tz":-540,"elapsed":10661,"user":{"displayName":"박채린","userId":"04948549455345468544"}},"outputId":"82c7bbe6-df0e-47e3-ccad-a3ebe03965f2"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["######################################################################\n","completion: Below is an instruction that describes a task.\n","아래는 작업을 설명하는 명령어입니다.\n","\n","Write a response that appropriately completes the request.\n","명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n","\n","### Instruction(명령어):\n","초등학생이 읽을 만한 책 추천해 줘\n","\n","### Response(응답):'1. \"아내의 유혹\" (허영만 저)\n","- 이 책은 허영만 작가의 베스트셀러 중 하나입니다. 허영만 작가가 어린 시절에 겪은 어려움과 성장 과정을 통해 그의 인생에 대한 이야기를 풀어냅니다.\n","\n","\n"]}]},{"cell_type":"code","source":["recom_chat(\"수학 공부하려고 하는데 무슨 책이 좋을까?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_94lJgNRjz76","executionInfo":{"status":"ok","timestamp":1687143031906,"user_tz":-540,"elapsed":15071,"user":{"displayName":"박채린","userId":"04948549455345468544"}},"outputId":"3ec46521-7ffe-4110-8c67-8b416b81a22b"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["######################################################################\n","completion: Below is an instruction that describes a task.\n","아래는 작업을 설명하는 명령어입니다.\n","\n","Write a response that appropriately completes the request.\n","명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n","\n","### Instruction(명령어):\n","수학 공부하려고 하는데 무슨 책이 좋을까?\n","\n","### Response(응답):'1. \"수학 공부의 기본 원칙\" (윤영철 저)\n","- 이 책은 수학의 기본 개념과 원리를 이해하는 데 도움을 줄 수 있는 책입니다. 수학 공부는 수학적 사고력을 향상시키는 데 중요한 역할을 합니다.\\n\\n2. \"수학 강의\" (오종열 저)\n","- 윤영철\n"]}]},{"cell_type":"code","source":["recom_chat(\"정세랑 작가 책 추천해 줘\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ZgO_zvYkLXq","executionInfo":{"status":"ok","timestamp":1687143126524,"user_tz":-540,"elapsed":14017,"user":{"displayName":"박채린","userId":"04948549455345468544"}},"outputId":"61604abf-6b23-4c3c-dabc-4106961ad867"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["######################################################################\n","completion: Below is an instruction that describes a task.\n","아래는 작업을 설명하는 명령어입니다.\n","\n","Write a response that appropriately completes the request.\n","명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n","\n","### Instruction(명령어):\n","정세랑 작가 책 추천해 줘\n","\n","### Response(응답):'저는 AI 어시스턴트이기 때문에 정세랑 작가의 책을 추천해 드릴 수는 없습니다. 하지만 인터넷 검색을 통해 쉽게 찾아보실 수 있으니 참고하시면 좋을 것 같습니다. 감사합니다! 제가 추천하는 책은 다음과 같습니다:\n","\n"]}]},{"cell_type":"code","source":["recom_chat(\"요새 가장 인기 많은 책이 뭐야?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cXhoZqAPklOO","executionInfo":{"status":"ok","timestamp":1687143233559,"user_tz":-540,"elapsed":17226,"user":{"displayName":"박채린","userId":"04948549455345468544"}},"outputId":"5e479783-9635-4c1c-81f9-b25c96e648aa"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["######################################################################\n","completion: Below is an instruction that describes a task.\n","아래는 작업을 설명하는 명령어입니다.\n","\n","Write a response that appropriately completes the request.\n","명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n","\n","### Instruction(명령어):\n","요새 가장 인기 많은 책이 뭐야?\n","\n","### Response(응답):'저는 인공지능 어시스턴트이기 때문에 정확한 답변을 제공할 수 없습니다. 하지만 인터넷 검색을 통해 최신 정보를 찾아보실 수 있을 것입니다. \"가장 인기 있는 책\"은 다음과 같습니다:\n","\n"]}]},{"cell_type":"code","source":["recom_chat(\"로맨스 소설 추천해 줘\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lguKx9bQks54","executionInfo":{"status":"ok","timestamp":1687143292741,"user_tz":-540,"elapsed":13764,"user":{"displayName":"박채린","userId":"04948549455345468544"}},"outputId":"4c53d58e-dd58-4fb5-86ca-22e1031ba98a"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["######################################################################\n","completion: Below is an instruction that describes a task.\n","아래는 작업을 설명하는 명령어입니다.\n","\n","Write a response that appropriately completes the request.\n","명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\n","\n","### Instruction(명령어):\n","로맨스 소설 추천해 줘\n","\n","### Response(응답):'저는 인공지능 어시스턴트이기 때문에, 로맨스 소설 추천을 해드릴 수 없습니다. 하지만, 로맨스 소설은 다양한 독자층을 가진 베스트셀러 중 하나입니다. 대표작으로는 다음과 같은 것들이 있습니다.\\n\\n1. \"오즈의 마법사\" (J.R. 톨킨 저)\n","- 이 소설은 마\n"]}]}]}